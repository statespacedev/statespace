notes
============

archives folder
------------------------

1976, triangular covariance factorizations, thornton

1977, square root and ud covariance factorizations, maybeck

2003, ekf-spkf-pf review, merwe

2004, ekf-spkf-pf dissertation, merwe

2006, oregon bayesian estimation library, merwe

2008, ekf software appendix, grewal

2009, structural time series models and the kalman filter, jalles

2015, inferring causal impact using bayesian structural time-series models, broderson

books
-------

bayesian signal processing: classical, modern, and particle filtering https://www.amazon.com/Bayesian-Signal-Processing-Classical-Communications/dp/1119125456

kalman filtering: theory and practice https://www.amazon.de/Kalman-Filtering-Theory-Practice-MATLAB/dp/0470173661

time series analysis by state space methods https://www.amazon.de/-/en/James-Durbin/dp/019964117X

an introduction to state space time series analysis https://www.amazon.com/Introduction-State-Analysis-Practical-Econometrics/dp/0199228876

forecasting, structural time series models and the kalman filter https://www.amazon.de/-/en/andrew-c-harvey/dp/0521321964/ref=sr_1_5?dchild=1&keywords=harvey+time+series&qid=1618740688&sr=8-5

notes a
--------

in the summer of 1990 i was an astronomy undergrad at the university of texas in austin. in recent months the berlin wall had fallen and the hubble space telescope had finally reached orbit. i was living just north of campus. monthly rent was less than two hundred dollars and my landlady martha ann zively, eighty-three year old local-legend, lived directly overhead. mobile phones, notebook computers, and the web were all still somewhere over the horizon.

the previous fall, i’d started working as a research assistant with the hubble space telescope astrometry team. this group had members from the astronomy department, mcdonald observatory, the aerospace engineering department, and the center for space research. my supervisor was paul hemenway, an astronomer involved with all of these organizations, and with the european space agency and its hipparcos project. paul started off by explaining the group’s projects and where i might be able to help.

hubble was designed for very exact and stable pointing to minimize motion smear in its images. three optical interferometers were mounted on robotic arms in the hubble’s focal plane to provide feedback to the pointing control system. these fine guidance sensors were a cutting-edge solution given 70s and 80s technology, with its uneasy mix of the analog and digital eras, and exact calibrations were needed on-orbit to make the whole complex system work as intended.

we used asteroids. mcdonald observatory and csr efforts for hubble fgs calibration became a component of the texas minor planet project. with group members in the astronomy and aerospace departments, we refined asteroid orbit determinations to the point where predicted positions and motions were exact enough to use as ground-truth references for comparison with hubble observations. our primary tools were a dec vax/vms cluster at csr, the main astronomy department vax running bsd unix, various early sun workstations, and the eighty-two inch telescope at mcdonald. it could image asteroids onto glass photographic plates, given enough time. for the long exposures needed to gather enough light from these dim objects, we depended on a data general nova minicomputer and the suitcase sized cassegrain camera.

this was another sophisticated piece of custom-made analog hardware. for locking on to and tracking a guide star, it used an image dissector, a large photomultiplier tube attached to the camera’s side that looked a bit like an engine part. its circular field of view was divided into four quadrants. a guide star was manually positioned at the center and the closed-loop control system was activated. every second, with a loud mechanical knock, the system would bodily adjust the camera position and try to keep the guide star at the center. on a green phosphorescent screen, a fuzzy spot bounced about, now nearer the center of the cross hairs, now further out.

an observing night began a few hours before sunset. down in the control room, circling the building beneath the telescope floor, the minicomputer and its control programs had to be started. one could step out onto a catwalk at the base of the dome for a spectacular view of the shadows growing out across the high desert from the mountains. various obsolete computers and mysterious bits of hardware crowded the space. the curving walls, low ceiling, red leds, and glowing crt terminals completed the 60s sci-fi movie setting.

a stack of white envelopes, each containing a glass photographic plate, was waiting. to prepare, we worked in the control room using a command line program on the nova to generate telescope pointing information for a list of asteroids. using this mini may have been my last serious contact with the large eight-inch floppy disks. they were a vanishing breed by the late 80s. after jotting down notes for the planned observations, we took the plates up into the dome, where it was pitch black except for clouds of stars in the open slit. the telescope loomed overhead in the darkness and we cautiously climbed the stairs up onto the circular telescope floor. it rose and descended in order to stay near the camera as the telescope and dome were moved and one could easily step off high above the dome floor.

we positioned the floor so the camera hung at eye level. sliding out the plate-cover opened a rectangular frame of stars, with the silhouette of the telescope secondary mirror housing and its support struts high above. mcdonald maintenance staff had mounted the camera to the telescope and connected power cables. fine tuning was always needed, and the telescope itself had to be focused. this meant adjusting the position of the secondary mirror within its housing. a rocker switch on the telescope hand controller activated a motor to move the secondary inward or outward. the exact determination of focus was old-school, using a knife-edge.

in the telescope’s focal plane, all of the light from a star converges through a single point. when a knife-edge cuts through that point, the light from the star is cut off instantly. if the knife-edge dims the star gradually, then the secondary mirror position needs to be adjusted. we wanted the point of instant cut off to be where our photographic plates were held by the camera. we fastened into the camera a special metal frame mounting a straight knife-edge, then adjusted the secondary mirror position while watching the cut off of bright stars. if there was a bit of spare time, the metal frame could be replaced with another holding the eyepiece, a heavy glass lens requiring both hands to lift. peering inside, one saw directly a mysterious world of red or green nebulas or spiraling galaxies...

once the telescope was ready, we could prepare the camera. with the small field of view of the telescope, asteroids moved fast relative to the sky over an interval of around ten minutes. each asteroid was a bit different, and various orbital characteristics had to be taken into account. the direction and rate had already been computed bz the nova, and now the camera body was rotated in its mounting and programmed to move at a rate such that the target would appear to be motionless.

the rear-surface of the image dissector was a round crt screen divided into four quadrants. light from a star, cascading down through the photomultiplier tube, formed a green glow on the screen. we found a guide-star near the target and centered it in the screen. with the tracking loop activate, the camera position was updated once per second with a mechanical click, keeping the star at the center of the screen.

an asteroid exposure began with guide-star tracking. then the steady clicking of the control loop would go silent for a period. the sky would turn while asteroid-light built up a spot on the photograph. then the clicking would resume. the result was a dumbbell shape for stars, with two circular peaks connected by a trail. the asteroid was a trail with a circular peak at its midpoint. these peaks and trails became visible the next day when we developed the plates. each had many dumbbell shaped stellar trails - short or long, thick or thin - and at the center a single ufo shaped asteroid-image.

the next steps were extracting information from the plates, and improving knowledge of the asteroid orbits. these took place back in austin, where the center for space research and department of aerospace engineering become involved. their expertise in orbit determination played an important role in the hubble astrometry team. the space age was roughly thirty years old, and its first generation led the center for space research - ray duncombe, byron tapley, and bob schutz.

first the plates had to be measured using a scanner and minicomputer in the scanning room, hidden behind the astronomy department library on the thirteenth floor of robert lee moore hall - better known simply as rlm. i spent many hours in the scanning room. it was a meditative kind of place, cool and dark, with a steady drone from the electronics fans. the long back wall was covered with cabinets containing thousands of glass plates, including historic sets of survey plates from palomar and the european southern observatory, alongside many plates from mcdonald. black plastic sheets shielded the end of the room from stray light, and at the center of this cave sat the pds microdensitometer.

this was a machine for mechanically scanning photographs - an interesting time capsule of analog-era technology. light from a bulb was focused into a beam downward through a mechanically driven stage with position encoders. a photometer below the stage measured the transmitted intensity while the stage moved in a raster pattern. sampling of the photometer and encoders was done by a very early, mini-fridge sized and rack-mounted sun workstation.

my first observing run at mcdonald was in may or june of ninety. chat among the astronomers was about problems with hubble that were repeatedly making headline news. i remember clearly there was still lots of discussion of the high gain antennas, because news of the catastrophic error in the primary-mirror hadn’t yet leaked out. overhearing the veterans during those days at mcdonald was an early revelation about the realities of science and technology.

paul and i had made the eight-hour drive to west texas. we spent three or four nights making plates with the eighty-two inch, and then made the drive back to austin. texas summer heat was just beginning to get intense, and after our return i made the sweltering walk over to rlm and happily settled into the cool darkness of the scanning room. my little apartment was already uncomfortably warm during the day, even with the air conditioning running.

our plates from mcdonald were roughly the size and shape of writing paper. the glass was fairly thin, and fragile enough that taking extra care was natural. held up against a background light, the star and asteroid trails were small dark smudges. with the plate secured to the pds scanning stage, and looking across the plate’s surface, one could see the dull black trails of photographic emulsion on the surface of the glass. the control software on the workstation had to be told what areas on the plate to scan. this meant moving the scanning beam about the plate, manually steering the stage and noting coordinates.

at the top of the pds, roughly at eye level, was a circular glass screen showing a magnified image of the plate illuminated by the scanning beam. this was essentially a microscope projecting directly onto the screen. individual grains of photographic emulsion were visible, and when the beam was near a star trail it appeared as a fuzzy black worm. the stage was adjusted using two finely geared knobs, and the coordinates of the scanning beam were shown by two sets of red leds on the pds console. the corners of a rectangle about a star trail were the coordinates for a raster scan, and were entered in manually at the workstation keyboard.

the workstation was a tall rack standing in the back corner and mounting a mini-fridge sized early sun box. on a table beside the rack was an extremely heavy crt monitor showing one of the first primitive unix guis i got to know well, the sunview precursor to x window. it already had the slightly antiquated feel of an earlier era. a scanning session meant creating a set of digitized raster files, one file for each trail scanned by the pds, archived on 9-track half-inch tape. a group of files, say thirty to fifty for a plate with a good exposure and lots of stars, was created in the filesystem of the workstation and then written to tape using its sibling above on the sixteenth floor, which had the tape drive. the shift over the border from analog to digital took place in the 70s style electronics connecting the pds to the workstation.

a few days after scanning those first plates, i went to meet with paul and ray duncombe in wrw, the aerospace building. i can clearly remember stopping in the texas sun. overhead was the typical hard blue summer sky with little white clouds, and i was already sweating just seconds after stepping outside. exactly which stars were on the plates? how could we identify them in order to determine the position of the asteroid? was there a program on the astronomy or aerospace computers to do this? the answer was no. there wasn’t an easy or obvious solution. helping to figure out a practical method for our particular plates was part of my job. not that an undergrad was expected to be able to solve the problem, but at least to get a feel for the questions involved. how did one go about recognizing stars? humans could do it, but could an 80s computer system?

thirteen years later, i entered the aerospace graduate program and went to work in the icesat group at csr. bob schutz was my boss for the next eleven years. my job concerned star trackers - modern descendents of maritime sextants for celestial navigation - and inertial sensors. once again i was dealing with images containing a scattering of unknown stars. within aerospace, it’s a classic problem with a memorable name - the lost in space problem. given an image of some stars, what are we looking at? aerospace has its own perspectives, cultural bents, and tools. astronomers don’t generally think in terms of three-dimensional unit vectors, rotation matrices, quaternions, and vector-matrix notation. it was soon apparent that the concerns and methods in aerospace were more widely applicable than those in astronomy - bringing together optimization, control, data fusion, high performance computing, and machine learning to solve real-world problems.

within weeks of beginning, star identification was again one of my major concerns. once again the first question was whether a practical solution was available. i checked back with people in the astronomy department, after being out of touch for nine years or so. pete shelus from the hubble astrometry days was a member of our csr group and pointed me in the right directions. there was a strong sense of continuity - here was a problem that really needed addressing. the obvious differences were that computing hardware was now more powerful, and digital imaging was now standard. there was no longer an analog to digital divide to cross, everything was in binary.

icesat’s control system usually made it straightforward to predict which stars each image contained. this wasn’t obvious or straightforward at first and it took effort and thought to really understand the data coming from the spacecraft. there were four star imagers of three different hardware-types onboard, all sampling at ten hertz or more. these were classic 80s star trackers and did not provide star identifications. there was also higher-frequency angular-rate data from the inertial unit.

a pointing vector could be estimated for each star-image, and it was usually enough to check whether star-images with appropriate brightnesses were near their predicted positions. brightness information tends to muddy the star identification problem because it’s difficult to either measure or predict for a particular imager. images have better geometric information than brightness information - an astronomer interested in brightness does photometry with dedicated sensors, not with imagers.

an additional check was that angles between observed star pairs matched predictions, and one of my first objectives was to model errors in these angles from flight data. focusing on star pairs is a big step in the direction of looking at star triangles and patterns. i quickly began exploring the literature on star identification and related topics - and discovered a self-contained little intellectual world.

its roots go back to ancient celestial navigation. the technology has evolved continuously from the age of sail. in the second world war many large aircraft had a bubble window facing upward for a navigator to make stellar observations. after the war, computing and imaging brought increasing automation. the cold war created an enormous propulsive force behind the technology. many people became uneasily aware of guidance systems. while most of the funding ended up in integrated circuits and inertial guidance sensors, automated star tracking quietly matured in parallel. star trackers are critical for spacecraft, and are still used on certain high altitude aircraft. the classical period, when the fundamental concepts were sketched out, was the sixties through the eighties - the high cold war era - with many scientists switching over into industry as direct government funding decreased.

it soon became clear that there were still no easily-available solutions for the star identification problem. apparently, each time star identification software had been developed, it’s been for a classified or industry project. if you were serious about star identification, you probably wanted to sell star trackers. that’s a mature industry now, with plenty of sellers and not a lot of buyers - there’s little motivation to think that way anymore, and i was soon meeting with the university intellectual property office concerning open source licensing.

another thirteen years passed. excitement was growing about advances in neural networks, especially at google, which had just open sourced tensorflow. for a number of reasons, it was clearly time to tackle the problem directly, using both geometric and machine learning methods in parallel.

the concept was to start from scratch as a github open source project, integrating tensorflow from the beginning. this meant working in c++ eigen and python numpy. the only external input was to be a list of star positions. nasa’s skymap star catalog was an ideal source. skymap was created in the 90s specifically for use with star trackers. we’d used it extensively for icesat, even collaborating where possible with its creators. when hubble was launched, one of its early problems was bad guide stars. as part of the overall hubble recovery effort, nasa pushed skymap forward as an improved description of the sky as seen by standard star trackers.

skymap is simply a list of star positions, so how does one generate a star image? the core problem is searching for neighbors of an arbitrary point on a sphere. for example, given a list of points on earth, which of the points are near a particular latitude and longitude? the usual answers involve dividing the sphere up into tiles, transforming and subdividing, etc. even a square-sky is not unheard of. a more dynamic and flexible approach was published by daniele mortari. it’s closely related to lookup and hash tables, but has some unique and interesting quirks.

view stars as unit vectors with three coordinates between -1.0 and +1.0. we’re searching for stars within small ranges of each coordinate. picture three thin rings on the sky, one centering on each coordinate-axis, and finding the stars inside the small region where the rings intersect. we’re left with three independent searches for small ranges of values, followed by an intersection of the results. each search is performed on a separate precomputed key-value table, with sorted keys from -1.0 to +1.0 and values representing star labels. performance can be improved by fitting a curve to the sorted floating-point keys and using it to calculate the low and high indexes into the table, creating something like a ranged-search hash-table with the fitted curve acting as a hash function.

cultural differences between machine learning and aerospace became apparent. to oversimplify, machine learning wants to be about images, while aerospace wants to be about three dimensional unit vectors. images appear more practical in many contexts, unit vectors are ideal geometrically. over roughly eight months, a higher-level image interface organically grew over the lower-level unit vector geometry, and a curious sequence of coincidences took place.

standard 90s star tracker images were eight degrees or 28,000 arcseconds per side - roughly sixteen times the apparent diameter of the moon. the hello world problem in machine learning, mnist, was standardized in the late 90s using data files and images with 28 pixels per side. adopting these standards resulted in star images with thousand-arcsecond pixels. this all happened quite accidentally. at first, actual mnist data files were simply overwritten with star images and then fed into standard machine learning processors. gradually advantages became apparent, beyond data file format compatibility. the implications are deeper than nice rounding properties, since they effectively mean low resolution - at the level of a toy camera or blurry mobile phone photo. by comparison, real star tracker images can involve sub-arcsecond resolutions.

low resolution makes the star identification problem more interesting. you’re forced to use global structures and patterns within an image, rather than localized features and heuristics. there’s simply less information available and you have to be more thoughtful about using it, even suggesting questions about how the human brain solves the problem. for example, a typical high-resolution aerospace algorithm might focus on the exact distance between a pair of stars, along with the angle to a third star. that’s clearly not how the brain identifies stars, but what is the brain in fact doing?

another nice coincidence is simple once you see it, but isn’t so obvious at first. when you want to identify a particular star in an image, it helps to shift the star to the image-center and make its presence implicit. there’s no point in including it in the image, what’s significant is the geometry of the other stars. it becomes the origin of the coordinate system, and if there’s another star nearby, as often happens in a low resolution image, there’s no confusion. in practice, the effects are even nicer than you might think. in a way, your getting an extra star for free while eliminating annoying coordinate transformations.

all the way back to 1990, it was clear that the shapes of triangles formed by a star field can be used to identify the stars. a few minutes thought always gave the feeling that this is somehow an iterative and even recursive approach. once you start thinking about triangles, they tend to multiply, which seemed uncomfortable. where does it end? skipping ahead to the answer, enlightenment came once the problem was stated as simply as possible. start with a set of star identities and iteratively set aside those that can’t be correct until only one remains. it’s brute force and hopefully there will be time later to find deeper insights. the main thing is, it works.

between the star-level and triangle-level is the pair-level. it’s the fundamental structural unit. soon after code for star images came code for pairs separated by less than eleven degrees on the sky. this was the fourth use of the key-value table described above, to represent nearly one million pairs as angles and member star identifiers.

the initial concept was to focus on groups of four stars instead of just three. for a triangle of three stars, adding a fourth provides significantly more information. you go from three edges to six, two of which are a shared pair. the tradeoff is significantly more complexity. for two adjacent triangles, the shared-pair represents a new type of constraint for which stars are possible. picture two sets of possible stars for the two triangles, kept in agreement via the shared-pair. with low resolution, this is harder than it sounds. there are too many pairs that meet low resolution constraints. a low resolution shared-pair just doesn’t provide enough unique information. it’s too ambiguous. in other words, at low resolution many of the skies triangles are similar.

eventually, the concept of the shared-pair became the focus. any pair of stars can be a shared-pair parent with many child-triangles. with the target star implicit in the center of an image containing ten other stars, there are ten shared-pairs that include the target star. each of these is the parent of nine child-triangles.

notes b
---------

this is an evolving discussion around state space models and processors - without going immediately to equations and code. as noted below, state space and coding are deeply intertwined and the emphasis has traditionally been on simply doing, not talking. we’re gonna try to change that. 

up front - what we’re trying to do here is directly inspired by sam savage’s flaw of averages. we’re even kind of assuming familiarity with the topics in flaw of averages. hard to give a stronger recommendation than that, so we’ll leave it there. to try to keep this whole high-tech mess as human as possible, i’ll bring in some personal backstory and motivations along the way - this means first-person stuff, which can be risky. easy to do, and hard to do well. we can point to flaw of averages again as an example of doing it well. let’s just see how it goes.

we can start off by giving a sketch of some of the projects to be discussed below - projects are what it’s all really about for me. there have been three biggies - work on two nasa science missions at the center for space research in austin over a span of roughly twenty-five years - and now work on maritime ship tracking in hamburg. curiously, all three involve vehicles, and all three are on a truly global spherical scale - flat earthers should stop right here. 

the earliest goes back to the late eighties and hubble space telescope - the research team i worked for, hst astrometry, was also involved with the esa hipparcos astrometry mission, predecessor of esa gaia. hubble and hipparcos were the first glimpses i got of the things we’ll be discussing below, and eventually caused me to move from astronomy to aerospace. that process took many years and was a bit surprising - engineering was not very appealing initially. there followed eleven solid years working on the ice, cloud, and land elevation missions - both icesat and icesat-2 - and working alongside the gravity recovery and climate experiment grace team. as another curiosity, grace has a big german dlr component. probably all a coincidence, but it’s possible that the german influence on the central texas hill country and austin played a role at several points in this story. in any case, we might discuss some things from these and related missions in what follows, and i’ve done some previous writing about it at statespace.dev/notes.

the third project - global maritime ship tracking - is probably of wider interest because of its direct connections with trade, finance, and economics. and there’s even a really nice topic within maritime for our purposes here - fuel burn. how effectively can we track, forecast, and detect anomalies in fuel burn, on a global scale? how do real-world gaps and issues in our sensor data interfere with these objectives, and how can we best improve our capabilities? these are exactly the kinds of things that state space processors are about - we’re deep in the heart of state space territory here, simple as that. after this whitepaper, you’ll hopefully understand what that means and why it matters. and there’s a bit more maritime discussion at statespace.dev/maritime. 

let’s get an overview of tracking and detection for this case to motivate what lies ahead. first of all we can say something about these two words - tracking and detection. in a nutshell, tracking means we’re sequentially following something forward through time, and detection means making decisions. the classic example is tracking a possibly hostile target and detecting the need to take action - but there’s very little difference here from tracking a price and detecting the need to buy or sell, or tracking a subsystem and detecting an anomaly. in this case, we’re tracking the fuel burn of ships around the globe in real-time while detecting ship and sensor anomalies. 

a convenient thing about this particular problem is that it means tracking vehicle motion in a single dimension, bypassing the geometric complications of two or three dimensions - its simplest form is distance traveled multiplied by some scaling factor. another nice thing is that it has lots of economically interesting structure - weekly and seasonal cycles, longer term trends, correlations with other signals - think about the different fuel burn patterns of tugs, ferries, or cargo carriers, and creating detectors for ships that are burning a lot of fuel, or geographic regions where we’re getting anomalous information. it’s reasonable to suspect that most ships, ship types, and geographic regions have fuel burn signatures. and they do.

where things get truly interesting is the fact that our incoming sensor observations are far from ideal - how we make use of the incoming information is our art. we’ll need to understand a bit about how ais works to begin exploring this topic - the maritime automatic identification system provides our sensors and physical signals, and amusingly enough, it’s actually quite bad at identifying ships. there’s no unique identifier, and no authentication - you can blast out whatever you feel like, and people do. it was a different era back in the day, and ais does its basic-survival job without worrying about the unintended consequences - next-gen follow-on systems such as vdes begin to address some of those. ideally we’d know engine speed, water currents, wave activity, weather, etc - but usually we don’t. 

we gotta make do with what ais gives us, and the reality with ais is - gaps. in the end, what we’ll see is that the nature of ais really puts the emphasis on distance - and there’s a generally applicable concept at work here that’s worth mentioning. as we tackle this problem, we want to begin by building a reliable foundation, then layer sophistication on top of that - to get the complete picture correct, even in lower resolution, before starting to zoom in on higher resolution details. to take advantage of the tradeoffs between low-resolution-low-noise versus high-resolution-high-noise. in other words, keep it simple at first and get it right.

for those of a mathematical bent - we want to get the constant and linear terms at least roughly right before worrying about higher order nonlinear terms - if our zeroth and first order terms break down, we’re doomed. the point a to point b distance is the minimum fuel burn - it can be higher, for example a ship that makes loopedy-loops along the way, but absolutely can not be lower. you can easily spend lots of effort on a fancy speed based approach and end up getting the result that a ship traveled half of the true distance - you missed a big part of the linear term. for fuel burn, distance traveled is the linear term - speeds, waves, and weather are all higher order corrections on top of the fundamental fact of how far a ship went point-to-point. fortunately, what ais does best is give us a point-to-point distance traveled signal.

we’re getting gappy distance observations from ais as a ship travels from port a to port b - it’s reliable inside the two ports, but between ports things are dicey. plotting distance versus time, we have a horizontal line at the start in port a and another horizontal line at the end in port b. in between, the line is sloping upward - distance and fuel burn are increasing - but something else is happening as well. there are shelves or stair-steps where the line goes horizontal for a period. these are ais gaps. inside a gap, all bets are off until the gap ends - the ship could be at full speed and steering in circles for all we know. not likely, but possible. how do we deal with that uncertainty as we track the ship, and how does it impact our detectors? clearly the uncertainty isn’t stable - it’s increasing dramatically inside the gaps, then dropping somewhat at the gap ends - but not vanishing, we’re not certain what really happened during the gap. our objective in the next discussion of fuel burn will be to better understand this evolution and flow of uncertainties revolving around ais gaps.

let’s sketch out where we’ll be headed, especially in the context of fuel burn and varying uncertainties. there are three main sections below - what state space processors are about, who uses them, and what they look like out in the real world. in each of these sections, we’ll try to have a subsection expanding on our fuel burn discussion. the first section tries to give a picture of what is meant by lower-level state space models and higher-level state space processors. the jargon isn’t too standardized here, and we’re definitely making strong use of the term processor - maybe there’s a better word for the purpose, but i haven’t been able to come up with one. at the least, i didn’t totally make this usage up - i picked it up from james candy’s book bayesian signal processing.

in the second section we take a look at historical, cultural, and organizational factors - cultural in the sense of profession and discipline. the birth place and native lands are in engineering, and it seems that now significant stuff is happening in economics as well - and there are some enlightening contrasts between the cares and concerns in these two fields. our concept here is to integrate and harmonize them in order to push our capabilities into new types of projects - we want to be at least somewhat out on the frontier, where we can add some value that will motivate people.

lastly we’ll take a look at more practical down-and-dirty aspects. one interesting thing about state space is that traditionally it’s been very much a do-it-yourself world - it hasn’t had big communities of prepackaged software users and big name-brands, as it were. this seems to be true for a number of intriguing reasons, and we’ll try to get a feel for those if possible. 

in this section we’re trying to get a basic picture of models and processors - they’re different creatures and we’ll try to get a handle on how they relate to each other and their distinct roles. in a nutshell, state space models are the skeleton for state space processors - the processors add completely new capabilities on top of the models. then we dive into the real heart of the matter - the evolution over time of uncertainty. this takes definite form as a feedback cycle within our state space processors. while the models are relatively simple and straightforward, much of the complexity and sophistication comes in the processors - dealing with uncertainty is where things get really interesting, and where we can really add some brains and value.

i often think of two stereotypical encounters - one from the business world, the other from the engineering and scientific world. the first is amusingly captured in sam’s flaw of averages, with the familiar middle manager type barking - give me a number, i’ve gotta have a number for production or sales or whatever. the other has the stereotypical professorial type asking - why give me just a number, where is your uncertainty, what’s your confidence in this number, any undergrad can make up a number. you can see where these metaphors are headed with regards to models and processors.

in state space, everything comes down to taking a single step forward in time - once we can do that, we simply repeat over and over. this involves two things - our beliefs about the world before receiving a new observation, and our beliefs after receiving a new observation. funny thing is, this all really got started way back in 1958, as we’ll see below - clearly demonstrating there’s nothing fancy or overly complicated happening here. it all started as soon as digital sensors and real-time computing became even remotely practical. probably because, what’s happening is simply a digital version of one of the oldest and most fundamental biological processes - closed-loop feedback control in which an organism or information processor repeats a basic-survival cycle of predict, correct, predict, correct, predict, correct. or in state space jargon - transition, update, transition, update, transition, update. 

think about what happens when you wake up late at night and wander half-asleep through a familiar but darkened room. or when you’re simply walking down some stairs in the dark. that moment of shock when you bump into something or miss a stair is your internal state space processor detecting an anomaly - your senses observed something significantly different than what your brain had predicted a split-second earlier, so you notice that particular cycle out of the endless repetition. probably it’d be a bit more insightful to say - putting one foot in front of the other since the paleozoic era. this type of adaptive feedback seems to go way back. i see we’re soon getting robotic pizza delivery - the control system in these will absolutely be a state space processor - so we’re definitely making progress, just not sure towards what. neither were the critters back in the paleozoic.

a common jargon term here is bayesian. for us, the most important meaning is that there’s a before and an after. latin and italics also tend to begin mysteriously popping up - prior, posterior, a priori, a posteriori. we’ll live simple and stick with - before an observation, and after an observation. when we begin looking at implementing processors to learn from sensor information, these simple phrases begin taking on new meaning. what raises state space models up into the higher realm of state space processors is the ability to learn from incoming observations - you’ve gotta have sensors and closed-loop feedback to have a processor. and the brains of a processor are the methods it uses to learn from its sensors - the examples we’ll see below are the kalman gain, and the importance weights of particles. 

from here forward we’re going to focus on state space processors rather than simply state space models - in a nutshell, the models predict, the processors predict and learn. while the term space space will imply processors below, it’s probably best to keep the picture of two levels in mind throughout - lower-level models, and higher-level processors built on top. basically all state space models have a set form hallowed by decades of tradition - once you’ve met one you’ve met them all. the only major differences are between linear and nonlinear models, and it’s obvious and straightforward - linear models are built from vectors and matrices, nonlinear models are built from vectors and vector-valued functions. a funny quirk of history is that engineers have the transition equation before the update equation and look backward in time, while economists have the update equation first and look forward in time - silly, but surprisingly frustrating when comparing two discussions of exactly the same thing side-by-side, and an inevitable source of implementation bugs. processors on the other hand take on radically different forms - from classical kalman filters, to modern sigma-point filters, and onward to next gen particle filters. 

what we’re really doing with each step - each repetition of the cycle - is evolving a shape forward in time. the shape represents our confidence and uncertainty about reality - what we know, or at least think we know - and more importantly what we don’t know. how can a shape represent all that? think of the age old rejoinder - how much are you willing to bet on that? inevitable answer - it depends! everyone will start qualifying and hedging when you hit them in the pocketbook. the trade-offs involved in that simple fact - translated into numbers - is where our shape comes from. and another aspect of this is the fact of life that if we do something many times, we’ll get many different paths in state space. the density of those paths gives us our shape as well. this is one of the places simulation opens new possibilities, as we’ll see below - computers are great at generating lots of paths, really giving body and solidity to our shape. 

what happens in the moment separating before an observation from after an observation? we learn something about how mistaken we were. the key thing here is to have a clear idea of how our observations depend on our beliefs, and the inverse - how our beliefs evolve with our observations. maybe we knew less than we thought, so we adjust our shape. the question is how to incorporate the new information we’ve received. how confident should we be in it, and how should we adjust our beliefs?

this is where we can point at something concrete defining exactly what a kalman filter is - exactly what distinguishes a kalman filter from all other algorithms. a kalman filter has a unique equation that specifies how to learn - how to adjust our belief given a new observation - this equation prominently features a variable named the kalman gain, often represented as capital-letter k. if you come up with your own way to compute this gain - then give it your own name and call your new processor after yourself! fair warning - you’re going to have a hard time improving on the original, at least for applications where it’s already thrived for decades. fact is, the equation for computing the kalman gain wasn’t pulled out of thin air - it makes deep nature-of-reality sense.

if we look closely at a state space processor’s observational update, we’ll realize it has new components not present in a state space model. the model is there, for sure - it’s acting as the skeleton for a whole new nervous system of additional concepts that power the processor. now we’re approaching a deeper understanding of the differences between models and processors. in order for a processor to learn, it needs to have a representation of uncertainty and a way to use that uncertainty while incorporating new observations - it needs to know and use our shape! none of this is present in state space models - only in processors. and here we see why processors take on radically different forms, while models are essentially similar - there’s vast space for creativity in evolving models upwards into processors. this isn’t the place to begin exploring some of the different processor forms, but we’ll at least begin glimpsing some in passing below.

the focus here is on how uncertainty is understood by models and processors for the fuel burn topic - this should be a good context for getting an intuitive feel for the differences between the two. every state space model has a representation of uncertainty of course - two actually - they’re obvious top-level terms, one in the transition equation and one in the observation equation. let’s explore what these two model components are doing, before turning and comparing them with uncertainty representations in kalman and particle processors.

uncertainty terms in the model concern inputs into the system - things that are external to the system, or are unknown-unknowns within the system and so also in some sense beyond or external to it. standard jargon for the term in the transition equation is process noise, and this has some revealing history. in many control and industrial process engineering projects, the focus is on what’s referred to as ‘the plant’ or ‘the process’, as in industrial physical-plant, industrial process, etc. it’s not uncommon to hear control engineers discussing a missile or spacecraft and mention a plant - which is kinda ridiculous, but in their minds they’re simply seeing the standard control system equations and using the jargon they’ve grown up with. process noise means, effects of the stuff we don’t know about inside our system or ‘process’ - otherwise we’d predict it. the term in the observation equation is simply called observation noise and has the same interpretation - it’s the effects of stuff we don’t know about in the observations, otherwise we’d predicted it.

how do these handle the evolution and flow of uncertainties revolving around ais gaps? well, they don’t - at least not directly. we intuitively know our overall uncertainty is growing and shrinking in fairly complex adaptive patterns because of the gaps - and that ain’t gonna come easily from two puny linear terms. we need something more - here’s another signpost pointing upwards toward state space processors. this is setting things up a bit too tidely - there are undoubtedly possibilities with state space models alone. there’s no question, however, that we want the capabilities of a processor here - one of the things a processor has to include is a more general and powerful representation of uncertainty than what the process and observation noise terms provide.

it’s strikingly obvious with even a casual glance at the kalman equations - there’s a mysterious capital letter p popping in everywhere. in fact, there are entirely new equations concerned just with this p. this is the covariance matrix - the uncertainty. an uncertainty nervous system has grown on top of the skeleton formed by the state space model. this’s what we need to tackle ais gaps - those new equations are exactly about the topic of how the uncertainty evolves during transitions and updates. the story with particles is even more interesting, as the particles themselves swarm and evolve with the uncertainty shape - the ais gaps exert ‘forces’ on the particles, generating new behaviours.

in this second section we’re gonna take a look at historical, cultural, and organizational perspectives. i often think of a story my boss bob schutz told, about back-in-the-day, early in his career working with nasa. this was back in the sixties and he was already getting a leadership role, coming from byron tapley’s strong group at u.t. austin. bob was up at nasa goddard and was explaining kalman filtering to a nasa manager. this guy broke in with a bark - we don’t care about kalman filters around here! all we care about are coffee filters!

quick aside, just think about that for a sec - bob - and byron, who was right there beside us the entire time - were deep into everything during the very first generation of the space age. in some ways, that stuff just doesn’t seem so long ago - i’d say i’m solidly in the second generation, the one that came up in the seventies, eighties and nineties - let’s say, post-apollo. the really cool thing was that both generations were just jumbled up together and piled on top of each other there in austin. the center for space research in austin - good ol days indeed.

up front, as part of being focused on projects - the ideal is to always be seeing paths forward, to be making progress. what we’re looking at here is how to take what’s already going on and go a bit further - push onwards, into new unexplored types of projects. be at least somewhat out on the frontier, where we can add value, motivate people, and solve problems.

for our purposes here, state space was invented on a train waiting outside baltimore late one night in november 1958. what was it that rudolf kalman saw that night, and how has it evolved over the last sixty years? back in the early days the focus was very much on differential equations, especially on newtonian equations of motion. trajectories through time and space here in our everyday world were becoming an ever larger topic. not many people thought deeply about ballistic motion through earth’s gravity field in 1947. their numbers began increasing dramatically around 1957. at the same time, the concept of sensors as independent quasi-biological sources of signals, along with information theory, created a balancing force on the other end of the seesaw from the dynamics crowd. not coincidentally, the switch from mechanical analog systems to electronic digital systems was beginning in earnest. 

here are two oscillating and balancing forces - physical differential equation dynamics, and electronic digital information sensors. as one of my old physics professors loved to say - the real world is about oscillations, and what causes oscillations is return-forces. stabilization, feedback, counteraction. dynamics and sensors form an ideal feedback marriage, and it’s embodied in the two state space equations - transition, and observation. what is oscillating here is uncertainty - it takes on the nature of a thing in itself. as discussed above, a processor knows and uses our shape. between observations we’re relying on our model and our uncertainty is growing over time, when we get an observation we learn something and it decreases our uncertainty.

there are subtleties to explore here - like what about the uncertainties inherent in the observations, where do they come in? this is an area where we’ll begin to glimpse some of the deeper powers of the state space approach. before even looking at some of the things we can do with a state space processor, we can see that every state space model directly represents observation uncertainty - this is why there are two equations, not just one! on top of this skeleton, processors build more sophisticated representations of observational uncertainty as part of their efforts to learn from incoming data. processors make decisions - how to divy up confidence between sensors and states. picture a processor receiving observations from both a high-resolution camera and a low-resolution radar - it should often be more confident in the camera, until the lights go out! meanwhile, it should always be adjusting the size of the updates it makes - smaller updates when its more confident in its states, larger updates when its more confident in the incoming observations. here we see where the kalman gain gets its name - the word gain is just a jargony synonym for size.

for an example, let’s take a look at deep space navigation - ever wonder how the pioneer and voyager missions rendezvoused so nicely with all those outer planets on their way out of the solar system? the classic picture here is a plot of an ellipsoid together with a circle - the ellipsoid is where we’re confident our spacecraft will be, and the circle is jupiter. clearly we don’t want the ellipsoid overlapping the circle by too much. given a few careful course correction burns of our motors, we have some control over the ellipsoid. and we’re making as many observations as we can manage using our telescopes, gyroscopes, magnetometers, gravimeters, and radios. how do we fuse those observations and compute the burns to get where we want to be, while not getting where we don’t want to be? we’re gonna be working in state space, and we can go further and flat-out declare that the kalman filter is crucial for the sensor fusion we need - combining the wildly different confidences and uncertainties of all those different types of observations - and for the real-time control needed to make the burns. to simply throw out some of the common jargon, the engineers involved thought in terms of state transition matrices, reference trajectories, and covariance matrices. regardless of the jargon, the crucial results are embodied in that ellipsoid shape - it’s simply a two-dimensional representation of a higher-dimensional covariance matrix. 

it’s fair to say that applications of state space in economics should be noticeably different than in real-time digital control systems. and they are. in short, the emphasis seems to be on batch processing instead of sequential processing. in batch mode, many new things become possible - in particular, the whole world of monte carlo simulation and probabilistic programming. this is a difference to be celebrated - and in the future, the advantage will be with those that can blend engineering and economics, sequential and batch.

first, we can simply point out that the term smoothing is synonymous with the term batch - if you’re smoothing, you’re in batch mode. and we can see that strong physical laws and dynamical differential equations play a smaller role in economics than in engineering. the focus becomes on discovering the models and systems themselves, rather than on applying known physical models to known physical systems. this whole topic is termed system-identification in engineering, where it’s clearly often simpler - what are the truly significant characteristics of the systems and models for our particular problem-to-be-solved?

for an example we can look at how google does short-term economic forecasting in state space - their chief economist published a nice paper on this back in 2013. for our purposes, what’s really going on here is the generation of tons of simulated paths - they seem suitably wary of haters taking cheap-shots and cover their tracks under the rather startling jargon bayesian data augmentation. how do they get their tons of simulated paths? they use their state space model in batch mode and repeatedly search through various parameters to home in on a nice fit with other observed data - they have a primary input time series, and many other secondary time series. all this takes place in a kind of timeless hyper-reality having not much to do with sequential real-time processing - it’s batch all the way down, the primary and secondary time series are simply sitting there waiting to be used. we’re now exactly in the region of the jargon terms monte carlo simulation and markov chain monte carlo, and we’ve stumbled onto a bridge between two separate worlds - sequential processors and batch simulations.

on the surface, the observations here are simply the primary time series - but what about the many secondary time series, how do they fit in? one thing we suspect immediately - it’s questionable to call them observations unless we have a representation of their uncertainties. turning that around we can say - unless we have a representation of its uncertainty, we don’t have an observation. if uncertainty is an essential part of observations, what about the uncertainty of the primary time series? we have that, or at least we can get it - there’s a batch method entirely about representing the uncertainty of the primary time series, thereby making it a healthy set of observations. so, if the many secondary time series are not observations, what are they? in a nutshell, they’re potential components of the overall model that we’re trying to discover - this is system-identification in action. the question becomes, which of the many secondary time series are of real use as components of our model, and what’s the effect on our overall uncertainty shape?  

let’s make a quick digression to note something interesting about economics and engineering. we’ve already met rudolf and seen how he developed state space models and his processor, primarily by focusing on covariance matrices. at exactly the same time - late fifties - harry markowitz was incorporating covariance matrices into financial portfolio management - a story presented in full in flaw of averages and bernstein’s capital ideas. coincidence? we’ve gotta suspect not - it was all happening as soon as it was practical, and is probably simply simultaneous parallel evolution. but we’ve also gotta wonder what could have happened had harry met rudolf back in the day. rudolf’s focus on time series and sequential processing may have been a bit alien, but there were definitely similarities - deciding on portfolio selections on one side, and deciding on observation updates and control-system commands on the other side. 

this section could be a bit hazy - cultural and organizational aspects of maritime fuel burn sounds a bit odd, and could be tough to get much insight here, but maybe it’ll turn out to be useful for unifying the engineering and economics themes. one thing we can say for sure - fuel burn isn’t at either end of the spectrum - it’s somewhere in the middle between fast sequential mode and slow batch mode. we gotta suspect there are real advantages to both sequential and batch approaches - and that’s great for us since we’re motivated to look at things from multiple perspectives and find new value. let’s jump right in with a batch mode discussion, followed by a sequential mode discussion.

from an economic perspective we’ll be interested in the concept of system identification mentioned above - what components is it reasonable to have in our system and what value can we extract from them? this is climbing upward to a meta or hyper level - looking down on our models and processors as a system in and of itself. batch mode is completely natural and organic here - we’re entering more platonic regions. in batch mode we can immediately make use of the structural state space models coming from economics - see durbin’s time series analysis by state space methods. this is a big deal by itself - opening up the cycles, trends, and time series correlation topics. structural state space models don’t do much for our ais gaps - but we can note that one of the big concepts in batch mode is monte carlo simulation, and clearly simulation will be a powerful tool for the ais gaps topic. in batch mode, we can go way beyond just sequential monte carlo - the whole simulation world opens up. for now i’ll leave this as a topic to be explored. 

from an engineering perspective, we’re tempted to turn the topic upside down - making it about real-time tracking and detection of ais gaps and anomalies in our system, as much as fuel burn and distance travelled. let’s quickly take a deeper look at the ais story. when it was developed back in the nineties, using solidly seventies and eighties tech and protocols, they were picturing basically line-of-sight ship to ship radio communication for automatic collision avoidance. it’s actually really interesting stuff - this was an early example of a self-organizing and resilient network, and the aviation ads-b system is following directly in ais’s footsteps. shore base stations also mesh in, and it is possible for a local mesh or cell to extend over the local horizon - it’s all adaptive and flowing. 

what wasn’t really expected is that satellites would be used to pick up the signals in an effective way, making the ais system somewhat global in nature - we have to carefully qualify that because it wasn’t designed to be global and there are strong limitations in practice. it’s with bemused admiration that we can salute it as the world’s first global self-jamming network. so now we have two broad types of receivers gathering ais signals - terrestrial receivers, often around ports and shipping lanes, and satellites. all loosely federated together via the internet. unintended consequences are abounding here, it’s really a fascinating case of tech evolving far beyond anyone’s control or design.

let’s set aside the satellite topic for now, and picture all the ships and terrestrial receivers around the globe - and now let’s picture a sequential processor for tracking and detection of both. our system is composed of both ships and terrestrial receivers. ais gaps are now positive information about our system, not a negative lack-of-information - gaps can tell us something new about our system. maybe an important receiver in a major port just went down, or our receivers near a particular shipping lane are experiencing degraded performance for whatever reason. there’s a wealth of information available here when we have eyes, and processors, to see.

lastly we’ll take a look at some down-and-dirty aspects of state space processors. whichever world you’re in - engineering or economics - you’re going to run into a handful of standard approaches to working in state space. they’re all melded with the digital world, born with and from digital sensors and computers. in short, working in state space and programming are one and the same thing - if you don’t want to do a bit of coding, you can pretty much forget about it. the fundamental language here is vector-matrix linear algebra - beyond that underlying fact it’s just minor differences of dialect, and again you can pretty much forget about it without linear algebra.

we’ll wrap all this up in our concept of a processor - a state space implementation running on digital hardware performing floating-point linear algebra operations, accepting digital sensor inputs, and producing digital outputs. what’s of real interest here is that there are a few generations of processors - roughly paralleling the growth in capabilities of sensing and computing hardware. what we can do here is look into some of the bigger practical challenges with implementation on real sensors and computers.

back around 1960 the vacuum tube era was giving way to transistors and integrated circuits. some impressive processors combining sensors and computation were already out there - it’s important to keep the sage system in mind, this wasn’t the dark ages by any means. and those types of systems were rapidly being shrunk down to form the guidance, navigation, and control systems of the first icbms and the apollo guidance computer. the semiconductor and integrated circuit industries weren’t just about free-market capitalism - it was cold war government planning and funding all the way down the line. and along with jamming-proof inertial guidance systems, there needed to be autonomous algorithms - so the motivation and the money were there with acronym-heavy customers like rand, darpa, sri, mitre, jason, nasa.

hardware capabilities were limited - maximum conciseness was the order of the day. everything was normal distributions - expected value vectors and covariance matrices. the big challenge was limited floating-pointing precision - bits were in short supply. it usually wasn’t a problem for representing the vectors - the problems came with the matrices. covariances involve the squares of both large and small values, and they have to be kept positive - negative uncertainties don’t make sense, so subtraction operations become dangerous. the solution was to implement covariance matrices in a way that doesn’t involve squared values and does stay inherently positive. some common jargon here is cholesky decomposition and square-root filtering. lots of effort was put into dealing with these issues during the sixties and seventies, and this points to an underlying theme here - we’re now more interested in how the covariance matrix evolves and changes over time. we’ve left the static batch world behind and are fully committed to the dynamic sequential world - things can seem to be getting more challenging.

at the same time though, things are actually quite simple in a way. we’re also committed to representing our picture of the world using the normal distribution - a simple symmetric peak, the good ol bell curve. the fit between normal distributions and vector-matrix operations is a bit mystical and mind blowing when you stop to think about it. on one side we have the world of probabilities, dice, spinners, cards, and gambling - all vaguely disreputable. it does seem that people are typically more drawn towards certainty, however illusory, rather than towards uncertainty. on the other side we have the world of linear algebra, which tends toward mathematical perfection. the clean cool laws of linear algebra are applied to the messy real-world - and maybe this is the source of a certain unease. we intuitively sense that some big simplifications are probably taking place here. often the approximations are quite explicit, as in the linearizations applied to nonlinear functions - but we have to suspect there are more subtle approximations involved as well, to make everything fit into orderly rows and columns.

now we’re going to get a feel for next gen approaches. hopefully it’s already clear where this is headed. if in the fifties we already had the digital sensors and computing needed to represent our shape with mean-value vectors and covariance matrices - can’t we represent our shape in much higher resolution and fidelity today? the answer is, particles - and the more particles your hardware can handle, the higher your resolution. we’re switching from the grainy black and white of 1958 to high definition. before anything else we need to be clear that there’s more happening here than a bunch of points making paths through state space - there’s a new concept that comes along with the particles. some common jargon here is sequential importance sampling, and we’re moving from sequential filtering toward sequential simulation. each particle is a position in state space, and associated with it is a weight representing the particles importance - the combination of positions (particles) and weights (importance) is how we express our shape and evolve it forward in time. in particular, the weights are the key to performing our observational update - just as the kalman gain was the real brains back in the good old days, the weights are the real brains here.   

a motivation for using particles and weights is that our shape can have more than one peak, something a normal distribution clearly can never do. and we can see right away one historical route leading directly from kalman to particles - start with a normal curve, then begin combining together multiple normal curves, and finally make all those normal curves narrower and narrower until each becomes a particle - the weights specify the heights of the normal curves as they become vanishingly narrow.

a really beautiful thing about the particle approach is that it’s ideal for seeing the big picture of what we’re really doing here - evolving our shape through time one step at a time. sure, at any time we can extract the usual types of traditional numbers representing expectations and uncertainties - there’s a menagerie of tools at hand. let’s face it - kinda boring. and there are tools for comprehending our shape in and of itself - a common jargon term here is kernel density estimation - a souped-up descendant of histogramming that converts our discrete particles and weights into continuous curves. but beyond all of that, there’s something more here. 

there’s a glaring difference between how kalman and particle processors learn from observations - it’s obvious in the equations and code. as mentioned above, kalman learning is scaled or tuned by the gain - but what’s being scaled? a difference - a subtraction operation between an observation and our prediction of the observation. a jargon term here is innovation - the amount of surprise contained in the observation. and a synonymous jargon term is residual - the amount of the observation that we don’t understand, otherwise we would have predicted it. we’ll run with the term innovation, and here’s the thing - innovations are nowhere to be seen in typical presentations of particle. what gives? first, let’s take a closer look at innovations in kalman, where they’ve come over time to play a huge role - evolving organically to cover anomaly detection, error modeling, and systems engineering in general.

let’s focus on kalman here, with possible digressions about particle - from either a fuel burn perspective or an ais gaps perspective, we don’t expect to be dealing with multiple peaks in our uncertainty shape. another thing we can say is that we expect plenty of outliers and anomalies in the observations - these just come with the territory in ais. this all has a kalman feel to it, and maybe we’ll find some interesting things to say about particle along the way. and let’s also focus on one particular example out of the many possibilities - we can really be creative here - i’ve come up with a two state processor, however, that demonstrates the basics while opening up some quite interesting capabilities.

from the very start, we’re at least as interested in our terrestrial receivers as we are a particular ship’s fuel burn. this is the turning-upside-down mentioned in the previous section - making the challenge about tracking and detection of ais gaps and anomalies in our system as much as fuel burn and distance travelled. the receivers are our sensors, the sources of our observations - and we’re going to focus very strongly on our observations here. we’ll use a processor with two states - the first concerning a particular ship’s distance traveled, the second concerning the observations and sensors themselves. 

the processor involves two types of observations, but this number is more for convenience and simplicity - there could easily be more than two. the observation equation ties the first type of observation strongly to the first state, and the second type of observation to the second state. when we get a type-one observation, the update most strongly influences the first state. when we get a type-two observation, the update most strongly influences the second state. 

meanwhile, we’re focused on how our uncertainty shape is evolving, and this is where things get interesting - the most important factor here will be the type-two observations, and more specifically the uncertainties of the type-two observations. we’re suggesting that the thing of critical importance can be not the state values, or even the observation values in and of themselves - but the uncertainties of the type-two observations, and the effects they have on the broader uncertainty. they’re pumping uncertainty into the broader processor - sometimes more, sometimes less - and reflecting or representing the performance our sensors. when a sensor is performing poorly, more uncertainty flows in - and the other way around. this is the purpose of the second state and type-two observations - they’re a channel for information concerning the sensors to flow through the processor.

let’s make this all a bit more concrete and give a flavor of practical implementation. both the first state and the type-one observations are simply distance traveled, as observed by any receiver, whether terrestrial or satellite. this is as straightforward as possible - we’re simply reading the point-to-point distance directly from latest ais position observation, pretty much end of story. we can add that the observation uncertainty for the type-one observations is pretty boring - it’s generally the gps position uncertainty and roughly stable, nothing very dramatic or informative. 

when we add the second state and type-two observations, interesting stuff starts to happen - we now have a coupled system involving a two-by-one state vector and a two-by-two covariance matrix. so what exactly are the second state and the type-two observations? this is where we start to be creative - they could be many things - we have a picture of the role we want them to play, and can easily come up with lots of possibilities. we’ll make a single concrete proposal here to illustrate, ending up with some actors who can play the roles we want. the plan is that sensor uncertainties will be flowing in via the type-two observations - the second state is a bit of a placeholder for now, though useful, as we’ll see. intuitively we can say that a particular sensor is performing better when we’re receiving more signals and worse when we’re receiving less - so the uncertainty will vary dramatically with signal strength - no signal means total uncertainty, strong signals means high certainty.

our proposal is that each type-two observation is tied to a single sensor, a single terrestrial receiver, and combines several ais messages over some possibly variable time interval. the interval length and number of messages will be important factors in the observation uncertainty - it’s giving us information about how well the sensor is receiving ais messages. and we can imagine using null type-two observations - placeholders signifying the absence of valid type-two information and carrying increased type-two uncertainty. since we’re combining several ais messages for each type-two observation, we might as well do something useful at the same time, like calculating a speed based on positions and time stamps - so this is pointing us towards a second component for our state vector. we’re ending up in an aesthetically happy place - our state vector contains distance and speed, and we’re getting sensor performance information via the type-two observation uncertainties.

what we’ve seen here is how kalman encourages experimentation and design - evolving naturally into systems engineering and trade offs between cost, schedule, and capabilities. this is no accident - it’d be good to trace the intertwined stories of kalman and systems engineering during the sixties, and maybe we’ll pursue that further soon. for now i’d like to take a quick look at what particle has to say to this type of thing. what we can focus on here is the fact that observation updates are inherently linear in kalman and inherently nonlinear in particle - this is related to the fact mentioned above that the kalman update is built around something referred to as the innovation, while no such thing appears in particle. to put it in down-and-dirty implementation terms, kalman updates are vector-matrix linear algebra - and there’s a lot of linear algebra tools ready at hand - while particle updates aren’t. the tools for working with particle are simulation based, not canned linear algebra stuff. what we’re often really interested in is understanding the sensitivity of our system to various design parameters, and linear sensitivities are naturally appealing - a small change to a design parameter should have a small effect on the system, not blow it up. but let’s face it - the real world is pretty darn nonlinear - sometimes even walking into your own kitchen can go totally nonlinear in the blink of an eye.

i’d propose that this is the kind of frontier we should be exploring for ways to add value. the linear vector-matrix stuff has been around since the sixties - now we have vastly more computing power for working with simulation. we’ve gotta suspect that this is where to focus our energy, rather than endlessly rehashing the same old dusty textbook stuff.

thanks for reading. hope you find this stuff as fun as i do. since it’s kept me fascinated for many years now - and looking forward to more of the same - gotta suspect it’s just plain flat out cool. let’s end with a quote from kalman himself, via the first chapter of grewal’s kalman filtering theory and practice.