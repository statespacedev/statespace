
Uncertainty has shape - a topography of possibilities - some likelier, some less. Over time the shape evolves with natural transitions and our own observations. We get an updated shape, and the cycle begins again. Here's the heart of state space modeling - two cyclical phases: transition, observation, transition, observation, transition, observation... State space models are tools helping us sequentially update our uncertainty - better models give truer shapes, cycle after cycle. They're sophisticated machines. Take your model and make a thousand random runs. You end-up with a shape - all the different possibilities. That shape is encoded in the sub-system genetics of your model. And it can be engineered...

Sequential filters on state space models - Kalman, Sigma-Point, Particle - are simply higher-level tools for putting our shapes to work. Our state space models encode our knowledge of reality - our filters are machines for learning and action. Kalman is oldest and simplest. It was immediately put to work where uncertainty was life-or-death - the Apollo Guidance Computer. It represents shape as Gaussian - simple as that. Sigma-Point adapts to more complex shapes by sampling at a few points. Particle is all shapes - swarms of samples that spawn, evolve, and die, taking whatever course is natural. They're Sequential Monte Carlo siblings of single-point-in-time Markov Chain Monte Carlo samplers.

Structural Time Series, or more plainly State Space Time Series, seem to cover an interesting region between lower-level pure state space models and higher-level sequential filtering. It seems that in the 90's STS was associated with Kalman, and that the new Tensorflow incarnations are moving towards non-sequential MCMC samplers. Will be interesting to watch for interplay between STS and sequential Particle samplers...


Let's cruise at orbital-level over the birth of state space in Cold War Aerospace - 1960 - over its mastery of Guidance, Control, Navigation, Signal Processing, Autonomous Vehicles and Robotics, etc - Traditional Machine Learning - and over its direct import from Engineering into Econometrics at the London School of Economics around 1990. We're up to 2012 now. A second generation at the London School of Economics brings their picture up-to-date. And then Google jumps in.

Google's Chief Economist is a bit interested in short-term forecasting - stuff like what web pages you'll be heading for today, how web searches interact with the global economy, etc - and he works in state space. The point here is that along with the usual econometric state components - trend, seasonal, cyclical - our state space model can also learn from a multitude of other time series - namely, all the stuff your doing on Google...

Then they give us an idea how Google's using state space to improve its marketing. Since it's possible to learn from all your Google activity, it's possible to forecast what you'd be doing in slightly different realities - a you that wasn't exposed to a particular ad campaign for instance... That second you is the B for Google's A/B Testing.


State space is a cycle of two phases - transition, observation, transition, observation, transition, observation. If we had infinite knowledge, we'd simply write down the true values at each step - nothing would be random. In reality, we model our limited knowledge via random disturbances - unmodeled effects - our unknown-unknowns. From this perspective, we suspect there’s more to the matter than simply plugging in a random number generator and letting fly. As pointed out in The Flaw of Averages, these unknown-unknowns tend to bite, especially when joining simulations together.

ProbabilityManagement.org is working to improve how we handle uncertainty. What we're interested in here is a framework and standard for plugging simulations together via SIPs - Stochastic Information Packets. It’s a no-brainer to cooperate in this effort and make it a core part of what we’re doing in state space. True, the short-term payoff is a bit hazy - this is just another of those moments in technology where there’s a Right Thing To Do, like back when you could choose between TCP/IP or DECnet. Now’s the time for a TCP/IP of Uncertainty - SIP for short.

Here’s some examples, including a state space model using SIPs and tracking the level of the Nile - it's simple as it’s possible to get, but say it were nonlinear and there's another nonlinear model tracking the Mississippi. Some of the disturbances are related - stuff to do with the Sun and global climate for example. Using shared SIPs for disturbances, our simulations experience the same unknown-unknowns. They’re taking place in a shared world.

